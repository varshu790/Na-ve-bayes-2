{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
        "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
        "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "ANS- This problem can be solved using conditional probability.\n",
        "\n",
        "Let's denote:\n",
        "- \\(A\\) as the event that an employee uses the health insurance plan.\n",
        "- \\(B\\) as the event that an employee is a smoker.\n",
        "\n",
        "The probability that an employee uses the health insurance plan is \\(P(A) = 0.70\\) (given as 70% or 0.70).\n",
        "\n",
        "The probability that an employee who uses the health insurance plan is a smoker is \\(P(B|A) = 0.40\\) (given as 40% or 0.40).\n",
        "\n",
        "We want to find \\(P(B|A)\\), which is the probability that an employee is a smoker given that they use the health insurance plan. This can be calculated using Bayes' theorem:\n",
        "\n",
        "\\[ P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\]\n",
        "\n",
        "We are given \\(P(B|A) = 0.40\\) and \\(P(A) = 0.70\\). Rearranging Bayes' theorem:\n",
        "\n",
        "\\[ P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\]\n",
        "\\[ P(A \\cap B) = P(B|A) \\times P(A) \\]\n",
        "\\[ P(A \\cap B) = 0.40 \\times 0.70 = 0.28 \\]\n",
        "\n",
        "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan (\\(P(B|A)\\)) is \\(0.40\\) or \\(40%\\)."
      ],
      "metadata": {
        "id": "_z_OftVp8GXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "ANS- The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the types of features they are designed to handle and how they model the data.\n",
        "\n",
        "1. **Bernoulli Naive Bayes**:\n",
        "   - Designed for features that are binary or Boolean (0 or 1).\n",
        "   - Commonly used in text classification tasks where the presence or absence of words in a document is represented as binary features (e.g., bag-of-words models).\n",
        "   - Considers only the presence or absence of a feature, not its frequency or count.\n",
        "\n",
        "2. **Multinomial Naive Bayes**:\n",
        "   - Typically used for features that represent counts or frequencies.\n",
        "   - Often applied in text classification where features are word counts or term frequencies in documents (e.g., TF-IDF vectors).\n",
        "   - Considers the frequency of each feature within each class.\n",
        "\n",
        "In Bernoulli Naive Bayes, the presence or absence of each feature is the focus, assuming that features are binary variables. Itâ€™s suitable for situations where the occurrence of a feature matters more than its frequency.\n",
        "\n",
        "On the other hand, Multinomial Naive Bayes deals with feature counts or frequencies, making it appropriate when the frequency or occurrence of features within each class is essential for classification.\n",
        "\n",
        "Choosing between these models often depends on the nature of the data and the problem at hand. If the features are binary, Bernoulli Naive Bayes might be more appropriate. If the features represent counts or frequencies (like word counts in documents), Multinomial Naive Bayes could be a better choice."
      ],
      "metadata": {
        "id": "GjIYqxY-8Sz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "ANS- Bernoulli Naive Bayes handles missing values by ignoring the missing features during the calculation of probabilities. When a feature value is missing for a particular instance, it effectively treats that missing value as if the feature were not present or absent.\n",
        "\n",
        "In practice, when working with missing values in Bernoulli Naive Bayes:\n",
        "\n",
        "1. **During Training**:\n",
        "   - Instances with missing values for certain features are excluded from the calculation of probabilities related to those specific features.\n",
        "   - The absence of a feature value (due to being missing) contributes neither to the presence nor to the absence of that feature for a given class.\n",
        "   - Essentially, the missing values are ignored, and the probabilities are estimated based on the available data.\n",
        "\n",
        "2. **During Prediction**:\n",
        "   - When predicting the class for a new instance with missing feature values, Bernoulli Naive Bayes calculates probabilities based only on the available features.\n",
        "   - Missing features for the new instance are treated as if those features are not present in the calculation of class probabilities.\n",
        "\n",
        "Handling missing values in Bernoulli Naive Bayes is somewhat implicit, as the model naturally accommodates instances with missing feature values by excluding those features during probability estimation and prediction. However, it's essential to preprocess the data appropriately by handling missing values before training the model to avoid potential biases or loss of information. Common strategies include imputation (replacing missing values with estimated values) or considering the absence of the feature itself as a separate category if suitable for the context."
      ],
      "metadata": {
        "id": "OqJvvgUX8eUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "ANS- Yes, Gaussian Naive Bayes can be used for multi-class classification.\n",
        "\n",
        "In Gaussian Naive Bayes, the assumption is that the features within each class are distributed according to a Gaussian (normal) distribution. This assumption holds for continuous data where the likelihood of the features given the class is modeled as a Gaussian distribution with a mean and variance specific to each class-feature combination.\n",
        "\n",
        "For multi-class classification problems, Gaussian Naive Bayes extends its application by using the Gaussian distribution assumption independently for each feature within each class. When new instances need to be classified into one of multiple classes, the model calculates the probabilities of the instance belonging to each class based on the Gaussian distribution of each feature within every class.\n",
        "\n",
        "During prediction for multi-class scenarios, Gaussian Naive Bayes computes the probability of an instance belonging to each class and assigns the class with the highest probability as the predicted class for that instance.\n",
        "\n",
        "So, while Gaussian Naive Bayes is often used for binary classification problems, it can indeed be adapted to handle multi-class classification by extending its calculations to accommodate multiple classes."
      ],
      "metadata": {
        "id": "ciME71SM9D6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "Implementation:\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score\n",
        "Discussion:\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "Conclusion:\n",
        "Summarise your findings and provide some suggestions for future work."
      ],
      "metadata": {
        "id": "lm4sALFt9MO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orWgLBV675up",
        "outputId": "74e1b4af-c579-47f2-cd16-be319dbe6065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "column_names = [\n",
        "    \"word_freq_make\", \"word_freq_address\",  # Add column names from the dataset documentation\n",
        "    # ... (Include all column names)\n",
        "    \"capital_run_length_average\", \"capital_run_length_longest\", \"capital_run_length_total\", \"is_spam\"\n",
        "]\n",
        "data = pd.read_csv(url, names=column_names)\n",
        "\n",
        "# Prepare data and target variable\n",
        "X = data.drop('is_spam', axis=1)  # Drop the target column to get features\n",
        "y = data['is_spam']  # Assign the target column as 'is_spam'\n",
        "\n",
        "# Initialize classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "# Perform 10-fold cross-validation for each classifier\n",
        "for clf, name in zip([bernoulli_nb, multinomial_nb, gaussian_nb], ['Bernoulli NB', 'Multinomial NB', 'Gaussian NB']):\n",
        "    scores = cross_validate(clf, X, y, cv=10, scoring=scoring)\n",
        "\n",
        "    # Print results for each classifier\n",
        "    print(f\"{name} Classifier:\")\n",
        "    for metric, values in scores.items():\n",
        "        mean_score = values.mean()  # Calculate mean score for each metric\n",
        "        print(f\"{metric.capitalize()}: {mean_score:.4f}\")  # Print mean score rounded to 4 decimal places\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VH2nt3eBR31",
        "outputId": "e718d979-e217-40d4-f024-f63d9f6c3c3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli NB Classifier:\n",
            "Fit_time: 0.0129\n",
            "Score_time: 0.0114\n",
            "Test_accuracy: 0.7614\n",
            "Test_precision: 0.7324\n",
            "Test_recall: 0.6316\n",
            "Test_f1: 0.6759\n",
            "\n",
            "\n",
            "Multinomial NB Classifier:\n",
            "Fit_time: 0.0093\n",
            "Score_time: 0.0083\n",
            "Test_accuracy: 0.5503\n",
            "Test_precision: 0.4468\n",
            "Test_recall: 0.5704\n",
            "Test_f1: 0.5004\n",
            "\n",
            "\n",
            "Gaussian NB Classifier:\n",
            "Fit_time: 0.0073\n",
            "Score_time: 0.0072\n",
            "Test_accuracy: 0.7164\n",
            "Test_precision: 0.8669\n",
            "Test_recall: 0.3299\n",
            "Test_f1: 0.4760\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}